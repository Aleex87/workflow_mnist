{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1ce2f8",
   "metadata": {},
   "source": [
    "# Training — Digits classification with PyTorch (MLP)\n",
    "\n",
    "## Goal:\n",
    "Train a simple neural network to classify digits (0–9) from `sklearn.datasets.load_digits()`.\n",
    "\n",
    "We will:\n",
    "1. Load the dataset\n",
    "2. Split into train/test sets (stratified)\n",
    "3. Normalize pixel values (between 0 , 1)\n",
    "4. Convert data to PyTorch tensors\n",
    "5. Create DataLoaders (mini-batches)\n",
    "6. Define a simple MLP model\n",
    "7. Train the model with a training loop\n",
    "8. Evaluate accuracy on the test set\n",
    "9. Plot training curves (loss over epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b191c1f",
   "metadata": {},
   "source": [
    "## Required libraries\n",
    "\n",
    "Let's start by importing the libraries we need for:\n",
    "- data handling\n",
    "- model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3836ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a3d60",
   "metadata": {},
   "source": [
    "Set a random seed for reproducibility and select the available device (CUDA / MPS / CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14335873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd88b7",
   "metadata": {},
   "source": [
    "## Step 1 — Load, split, normalize\n",
    "\n",
    "We load the dataset and perform a stratified train/test split.\n",
    "\n",
    "Then we normalize pixel values:\n",
    "- raw pixels are in [0, 16]\n",
    "- we rescale to [0, 1] by dividing by 16.0\n",
    "\n",
    "Normalization helps training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6296d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization:\n",
      "X_train min/max: 0.0 16.0\n",
      "X_test  min/max: 0.0 16.0\n",
      "\n",
      "After normalization:\n",
      "X_train min/max: 0.0 1.0\n",
      "X_test  min/max: 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data.astype(np.float32)   # (n_samples, 64)\n",
    "y = digits.target.astype(np.int64)   # (n_samples,)\n",
    "\n",
    "# Stratified split keeps class proportions similar in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Before normalization:\")\n",
    "print(\"X_train min/max:\", X_train.min(), X_train.max())\n",
    "print(\"X_test  min/max:\", X_test.min(), X_test.max())\n",
    "\n",
    "# Normalize to [0,1]\n",
    "X_train = X_train / 16.0\n",
    "X_test = X_test / 16.0\n",
    "\n",
    "print(\"\\nAfter normalization:\")\n",
    "print(\"X_train min/max:\", X_train.min(), X_train.max())\n",
    "print(\"X_test  min/max:\", X_test.min(), X_test.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0293a543",
   "metadata": {},
   "source": [
    "## Step 2 — Convert to PyTorch tensors and create DataLoaders\n",
    "\n",
    "PyTorch models work with `torch.Tensor`.\n",
    "\n",
    "We create:\n",
    "- a training DataLoader (shuffled)\n",
    "- a test DataLoader (not shuffled)\n",
    "\n",
    "DataLoader creates mini-batches, which makes training efficient and stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df779e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X shape: torch.Size([64, 64])\n",
      "Batch y shape: torch.Size([64])\n",
      "Batch X min/max: 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "X_train_t = torch.from_numpy(X_train)  \n",
    "y_train_t = torch.from_numpy(y_train)  \n",
    "\n",
    "X_test_t = torch.from_numpy(X_test)\n",
    "y_test_t = torch.from_numpy(y_test)\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "test_ds = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n",
    "\n",
    "# Quick check of one batch\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Batch X shape:\", xb.shape)\n",
    "print(\"Batch y shape:\", yb.shape)\n",
    "print(\"Batch X min/max:\", xb.min().item(), xb.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd40aafd",
   "metadata": {},
   "source": [
    "## Step 3 — Define the MLP model\n",
    "\n",
    "Architecture:\n",
    "\n",
    "- Input layer: 64 features\n",
    "- Hidden layer 1: 128 neurons + ReLU\n",
    "- Hidden layer 2: 64 neurons + ReLU\n",
    "- Output layer: 10 neurons (one per class)\n",
    "\n",
    "The output will be raw scores (logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "496ddb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DigitsMLP(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DigitsMLP(nn.Module):\n",
    "    def __init__(self, input_dim=64, hidden1=128, hidden2=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "model = DigitsMLP().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1b6fa",
   "metadata": {},
   "source": [
    "Let's test the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3610b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 64])\n",
      "Output shape: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# Take one batch\n",
    "xb, yb = next(iter(train_loader))\n",
    "\n",
    "xb = xb.to(device)\n",
    "\n",
    "logits = model(xb)\n",
    "\n",
    "print(\"Input shape:\", xb.shape)\n",
    "print(\"Output shape:\", logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626c9472",
   "metadata": {},
   "source": [
    "## Loss function and optimizer\n",
    "\n",
    "To train the model we need:\n",
    "\n",
    "- A loss function: measures how wrong the predictions are.\n",
    "- An optimizer: updates the model parameters to reduce the loss.\n",
    "\n",
    "For multi-class classification we use:\n",
    "\n",
    "- `CrossEntropyLoss`\n",
    "- `Adam` optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workflow_mnist (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
